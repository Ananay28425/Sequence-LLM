# Sequence-LLM Documentation

A terminal-first CLI for managing local LLMs via `llama-server`. Ensures **only one model runs at a time** with seamless profile switching.

## Table of Contents

- [Installation](./installation.md) â€” Setup and dependencies
- [Usage](./usage.md) â€” Interactive CLI, configuration, and commands
- [Examples](../examples/basic_workflow.md) â€” Sample workflows and patterns

## Quick Start

```bash
# 1. Install
pip install -e .

# 2. Launch interactive CLI
seq-llm

# 3. On first run, a config is created at:
#    - Windows: %APPDATA%\sequence-llm\config.yaml
#    - Linux:   ~/.config/sequence-llm/config.yaml
#    - macOS:   ~/Library/Application Support/sequence-llm/config.yaml

# 4. Edit the config to point to your llama-server and models

# 5. Use commands:
#    /brain    â€” switch to brain profile
#    /coder    â€” switch to coder profile
#    /status   â€” show active model status
#    /clear    â€” clear conversation history
#    /quit     â€” stop server and exit
#    (text)    â€” send chat message to active model
```

## Key Features

- ğŸ§  **Profile-based profiles**: Define named profiles (brain, coder, etc.) in YAML
- ğŸ’¬ **Interactive CLI**: Typer + Rich for beautiful terminal UI
- ğŸ”„ **Sequential loading**: Only one `llama-server` at a time; switching kills the old one
- âš™ï¸ **Auto-config**: Creates sensible defaults on first run
- ğŸ“Š **Status panel**: Rich-formatted output showing active model, port, health
- ğŸ›¡ï¸ **Safe shutdown**: Graceful SIGTERM â†’ SIGKILL on profile switch or exit
- ğŸ”Œ **Cross-platform**: Windows, Linux, macOS (uses `subprocess` + `psutil`)

## Architecture

Three core modules:

1. **Config** (`config.py`) â€” Load/validate YAML profiles, OS-aware config paths
2. **ServerManager** (`core/server_manager.py`) â€” Start/stop `llama-server`, health polling
3. **APIClient** (`core/api_client.py`) â€” Stream chat completions via `httpx`
4. **CLI** (`cli.py`) â€” Typer + Rich interactive loop

## Configuration Schema

```yaml
llama_server: "/path/to/llama-server"

defaults:
  threads: 6
  threads_batch: 8
  batch_size: 512

profiles:
  profile_name:
    name: "Display Name"
    model_path: "/path/to/model.gguf"
    system_prompt: "/path/to/system.txt"
    port: 8081
    ctx_size: 16384
    temperature: 0.7
```

See [Usage](./usage.md) for full reference.

## Tech Stack

- **CLI**: Typer + Rich (modern async/sync, beautiful UI)
- **Config**: dataclasses + pyyaml (simple, no Pydantic)
- **Process**: subprocess + psutil (cross-platform)
- **HTTP**: httpx (sync mode, streaming)
- **Tests**: pytest (unit tests with mocks)

## File Structure

```
sequence-llm/
â”œâ”€â”€ src/seq_llm/
â”‚   â”œâ”€â”€ cli.py              # Interactive CLI entry point
â”‚   â”œâ”€â”€ config.py           # YAML config + dataclasses
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ server_manager.py    # Process lifecycle
â”‚       â””â”€â”€ api_client.py        # Chat API streaming
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_config.py
â”‚   â”‚   â””â”€â”€ test_server_manager.py
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_config.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ index.md            # This file
â”‚   â”œâ”€â”€ installation.md
â”‚   â””â”€â”€ usage.md
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ basic_workflow.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Running Tests

```bash
pip install -e ".[dev]"
pytest tests/ -v
```

## Getting Help

- **[Installation](./installation.md)** â€” System setup, dependencies, llama-server
- **[Usage](./usage.md)** â€” Commands, configuration, troubleshooting
- **[Examples](../examples/basic_workflow.md)** â€” Real-world patterns

## Platform Support

| OS | Status | Notes |
|---|---|---|
| Windows | âœ… In dev, auto-config to `%APPDATA%\sequence-llm\config.yaml` |
| macOS | âœ… Auto-config to `~/Library/Application Support/sequence-llm/config.yaml` |
| Linux | âœ… Auto-config to `~/.config/sequence-llm/config.yaml` |

## License

MIT â€” See [LICENSE](../LICENSE)

