# Sample configuration for Sequence-LLM
# This fixture is used for testing configuration loading and validation

models:
  - name: "gpt-4"
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model: "gpt-4"
    temperature: 0.7
    max_tokens: 2048

  - name: "llama-local"
    provider: "ollama"
    base_url: "http://localhost:11434"
    model: "llama2"
    temperature: 0.5
    max_tokens: 1024

sequences:
  - name: "analysis_pipeline"
    description: "Multi-step analysis workflow"
    steps:
      - model: "gpt-4"
        prompt: "Analyze the following text for sentiment"
        input_var: "text"

      - model: "llama-local"
        prompt: "Summarize the analysis results"
        input_var: "analysis_result"

  - name: "code_review"
    description: "Code review sequence"
    steps:
      - model: "gpt-4"
        prompt: "Review this code for issues"
        input_var: "code"

      - model: "gpt-4"
        prompt: "Suggest improvements"
        input_var: "review_notes"

server:
  host: "127.0.0.1"
  port: 8000
  timeout: 30
  workers: 4

logging:
  level: "INFO"
  format: "json"
  output: "file"
